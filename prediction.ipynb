{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # use for data manipulation and analysis\n",
    "import numpy as np # use for multi-dimensional array and matrix\n",
    "\n",
    "import seaborn as sns # use for high-level interface for drawing attractive and informative statistical graphics \n",
    "import matplotlib.pyplot as plt # It provides an object-oriented API for embedding plots into applications\n",
    "%matplotlib inline \n",
    "# It sets the backend of matplotlib to the 'inline' backend:\n",
    "import plotly.express as px\n",
    "import time # calculate time \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # algo use to predict good or bad\n",
    "from sklearn.naive_bayes import MultinomialNB # nlp algo use to predict good or bad\n",
    "\n",
    "from sklearn.model_selection import train_test_split # spliting the data between feature and target\n",
    "from sklearn.metrics import classification_report # gives whole report about metrics (e.g, recall,precision,f1_score,c_m)\n",
    "from sklearn.metrics import confusion_matrix # gives info about actual and predict\n",
    "from nltk.tokenize import RegexpTokenizer # regexp tokenizers use to split words from text  \n",
    "from nltk.stem.snowball import SnowballStemmer # stemmes words\n",
    "from sklearn.feature_extraction.text import CountVectorizer # create sparse matrix of words using regexptokenizes  \n",
    "from sklearn.pipeline import make_pipeline # use for combining all prerocessors techniuqes and algos\n",
    "\n",
    "from PIL import Image # getting images in notebook\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator# creates words colud\n",
    "\n",
    "from bs4 import BeautifulSoup # use for scraping the data from website\n",
    "from selenium import webdriver # use for automation chrome \n",
    "import networkx as nx # for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
    "\n",
    "import pickle# use to dump model \n",
    "\n",
    "import warnings # ignores pink warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "fack_job = pd.read_csv('fake_job.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobell.it/70ffb52d079109dca5664cce6f317373782/...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serviciosbys.com/paypal.cgi.bin.get-into.herf....</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail.printakid.com/www.online.americanexpress....</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thewhiskeydregs.com/wp-content/themes/widescre...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL Label\n",
       "0  nobell.it/70ffb52d079109dca5664cce6f317373782/...   bad\n",
       "1  www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...   bad\n",
       "2  serviciosbys.com/paypal.cgi.bin.get-into.herf....   bad\n",
       "3  mail.printakid.com/www.online.americanexpress....   bad\n",
       "4  thewhiskeydregs.com/wp-content/themes/widescre...   bad"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fack_job.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>549341</th>\n",
       "      <td>23.227.196.215/</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549342</th>\n",
       "      <td>apple-checker.org/</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549343</th>\n",
       "      <td>apple-iclods.org/</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549344</th>\n",
       "      <td>apple-uptoday.org/</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549345</th>\n",
       "      <td>apple-search.info</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       URL Label\n",
       "549341     23.227.196.215/   bad\n",
       "549342  apple-checker.org/   bad\n",
       "549343   apple-iclods.org/   bad\n",
       "549344  apple-uptoday.org/   bad\n",
       "549345   apple-search.info   bad"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fack_job.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549346 entries, 0 to 549345\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   URL     549346 non-null  object\n",
      " 1   Label   549346 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.4+ MB\n"
     ]
    }
   ],
   "source": [
    "fack_job.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL      0\n",
       "Label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fack_job.isnull().sum() # there is no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of classes counts\n",
    "label_counts = pd.DataFrame(fack_job['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')#to getting alpha only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nobell.it/70ffb52d079109dca5664cce6f317373782/login.SkyPe.com/en/cgi-bin/verification/login/70ffb52d079109dca5664cce6f317373/index.php?cmd=_profile-ach&outdated_page_tmpl=p/gen/failed-to-load&nav=0.5.1&login_access=1322408526'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fack_job.URL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nobell',\n",
       " 'it',\n",
       " 'ffb',\n",
       " 'd',\n",
       " 'dca',\n",
       " 'cce',\n",
       " 'f',\n",
       " 'login',\n",
       " 'SkyPe',\n",
       " 'com',\n",
       " 'en',\n",
       " 'cgi',\n",
       " 'bin',\n",
       " 'verification',\n",
       " 'login',\n",
       " 'ffb',\n",
       " 'd',\n",
       " 'dca',\n",
       " 'cce',\n",
       " 'f',\n",
       " 'index',\n",
       " 'php',\n",
       " 'cmd',\n",
       " 'profile',\n",
       " 'ach',\n",
       " 'outdated',\n",
       " 'page',\n",
       " 'tmpl',\n",
       " 'p',\n",
       " 'gen',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'load',\n",
       " 'nav',\n",
       " 'login',\n",
       " 'access']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will be pull letter which matches to expression\n",
    "tokenizer.tokenize(fack_job.URL[0]) # using first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting words tokenized ...\n",
      "Time taken 4.705801000003703 sec\n"
     ]
    }
   ],
   "source": [
    "print('Getting words tokenized ...')\n",
    "t0= time.perf_counter()\n",
    "fack_job['text_tokenized'] = fack_job.URL.map(lambda t: tokenizer.tokenize(t)) # doing with all rows\n",
    "t1 = time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291747</th>\n",
       "      <td>biography.com/people/tex-avery-5540</td>\n",
       "      <td>good</td>\n",
       "      <td>[biography, com, people, tex, avery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430449</th>\n",
       "      <td>shockya.com/news/2011/09/24/nyff-2011-movie-re...</td>\n",
       "      <td>good</td>\n",
       "      <td>[shockya, com, news, nyff, movie, review, musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164150</th>\n",
       "      <td>defaultface.livejournal.com/</td>\n",
       "      <td>good</td>\n",
       "      <td>[defaultface, livejournal, com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219113</th>\n",
       "      <td>mylifeofcrime.wordpress.com/2006/03/01/this-da...</td>\n",
       "      <td>good</td>\n",
       "      <td>[mylifeofcrime, wordpress, com, this, date, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51015</th>\n",
       "      <td>www.lfy.ca/index.html</td>\n",
       "      <td>good</td>\n",
       "      <td>[www, lfy, ca, index, html]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL Label   \n",
       "291747                biography.com/people/tex-avery-5540  good  \\\n",
       "430449  shockya.com/news/2011/09/24/nyff-2011-movie-re...  good   \n",
       "164150                       defaultface.livejournal.com/  good   \n",
       "219113  mylifeofcrime.wordpress.com/2006/03/01/this-da...  good   \n",
       "51015                               www.lfy.ca/index.html  good   \n",
       "\n",
       "                                           text_tokenized  \n",
       "291747               [biography, com, people, tex, avery]  \n",
       "430449  [shockya, com, news, nyff, movie, review, musi...  \n",
       "164150                    [defaultface, livejournal, com]  \n",
       "219113  [mylifeofcrime, wordpress, com, this, date, in...  \n",
       "51015                         [www, lfy, ca, index, html]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fack_job.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\") # choose a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting words stemmed ...\n"
     ]
    }
   ],
   "source": [
    "print('Getting words stemmed ...')\n",
    "t0= time.perf_counter()\n",
    "fack_job['text_stemmed'] = fack_job['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fack_job.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Getting joiningwords ...')\n",
    "t0= time.perf_counter()\n",
    "fack_job['text_sent'] = fack_job['text_stemmed'].map(lambda l: ' '.join(l))\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fack_job.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization \n",
    "**1. Visualize some important keys using word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sliceing classes\n",
    "bad_sites = fack_job[fack_job.Label == 'bad']\n",
    "good_sites = fack_job[fack_job.Label == 'good']\n",
    "\n",
    "mixed_sites = pd.concat([bad_sites[:100],good_sites[:100]])\n",
    "mixed_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function to visualize the important keys from url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = good_sites.text_sent\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bad_sites.text_sent\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create cv object\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cv.fit_transform(fack_job.text_sent) #transform all text which we tokenize and stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature[:5].toarray() # convert sparse matrix into array to print transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(feature, fack_job.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data\n",
    "texts = mixed_sites['text_sent']\n",
    "labels =mixed_sites['Label']# 1 for positive, 0 for negative\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(['bad','good'])\n",
    "labels = le.transform(labels)\n",
    "# Tokenize and pad the sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=32, input_length=padded_sequences.shape[1]))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=2, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "loaded_model = pickle.load(open('fake.pkl', 'rb'))\n",
    "# Make predictions on new text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def predict(url):\n",
    "    result = loaded_model.predict([url])\n",
    "    print(result)\n",
    "predict('phlebolog.com.ua/libraries/joomla/results.php')\n",
    "predict('https://www.python.org/downloads/release/python-390/')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 791543,
     "sourceId": 1359146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29980,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
